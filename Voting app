1-->>
Write a common use-case, where you will use a daemon set instead of replica set.

Replica set is used to ensure that defined set of Pods are running all the time.
It is used to ensure that all nodes are running the copy of pod. They are commonly used for logging and monitoring for hosts, for example a node needs a service which collects the voting data and stores into database.
if you want to get data from each node, the best option is to schedule a container on every node that will gather these data from each individual node. 
if we schedule one container to gather data from all nodes, there is a risk of loosing from whole cluster when any node from whatever reason dies, but having a copy of the same container on every node will help.

######################################################################################

2-->>
Suppose you have deployed your application using a deployment controller. Assume the initial number of replicas is one. Write the steps needed to update a container's image using deployment, making sure that there is zero downtime.

Solution:
The Image of the Deployment can updated in the below steps

-> Deploy the Development
kubectl apply -f kubia-deployment-and-service-v1.yaml

->Check for the minReadySecond param value and set it to 10s so that there is no need to bring the pod downtime
            
kubectl explain deploy.spec.minReadySeconds
kubectl patch deployment kubia -p '{"spec": {"minReadySeconds": 10}}'
    
->Use command to set the image value in the Deployment

kubectl set image deployment kubia nodejs=luksa/kubia:v2

####################################################################################################################

3-->>
You have deployed an application, that is listening at port 8000. You used a replica-set to deploy it and created a NodePort service to make it accessible. But when you test it, somehow the application is not reachable at the port. Write down your approach and sequentially all the steps that you will take to find out the issue.

check node port
IP,specs
labels
selector in YAML file
check for the syntax

####################################################################################################################

4-->
VOTING APP 

Commands involve : 
-git clone https://github.com/ashishrpandey/example-voting-app
-cd example-voting-app/
-cd k8s-specifications/
-kubectl apply -f . 
-check the Status of all the pods 
- kubectl get all

output :

NAME                                     READY   STATUS    RESTARTS   AGE
pod/db-b54cd94f4-ntqpb                   1/1     Running   0          42h
pod/redis-868d64d78-xpr4p                1/1     Running   0          42h
pod/result-5d57b59f4b-5q28v              1/1     Running   0          42h
pod/vote-94849dc97-nd7ph                 1/1     Running   0          42h
pod/worker-dd46d7584-wwchb               1/1     Running   1          42h

NAME                         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
service/db                   ClusterIP   10.100.150.162   <none>        5432/TCP         42h
service/kubernetes           ClusterIP   10.96.0.1        <none>        443/TCP          42h
service/redis                ClusterIP   10.106.245.58    <none>        6379/TCP         42h
service/result               NodePort    10.98.21.83      <none>        5001:31001/TCP   42h
service/vote                 NodePort    10.100.81.185    <none>        5000:31000/TCP   42h

NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/db                   1/1     1            1           42h
deployment.apps/redis                1/1     1            1           42h
deployment.apps/result               1/1     1            1           42h
deployment.apps/vote                 1/1     1            1           42h
deployment.apps/worker               1/1     1            1           42h

NAME                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/db-b54cd94f4                   1         1         1       42h
replicaset.apps/redis-868d64d78                1         1         1       42h
replicaset.apps/result-5d57b59f4b              1         1         1       42h
replicaset.apps/vote-94849dc97                 1         1         1       42h
replicaset.apps/worker-dd46d7584               1         1         1       42h
[root@ip-172-31-10-134 k8s-specifications]#

###########################################################################################
Open browser and go the follwing URL 

http://18.140.196.252:31000 To go to the voting page, 
http://18.140.196.252:31001 to view the results of the voting

############################################################################################
Deleting the voting pod:
    
[root@ip-172-31-10-134 k8s-specifications]# kubectl get po
NAME                                 READY   STATUS    RESTARTS   AGE
db-b54cd94f4-ntqpb                   1/1     Running   0          2d10h
redis-868d64d78-xpr4p                1/1     Running   0          2d10h
result-5d57b59f4b-5q28v              1/1     Running   0          2d10h
vote-94849dc97-nd7ph                 1/1     Running   0          2d10h
worker-dd46d7584-wwchb               1/1     Running   1          2d10h

[root@ip-172-31-10-134 k8s-specifications]# kubectl delete pod vote-94849dc97-nd7ph
pod "vote-94849dc97-nd7ph" deleted

[root@ip-172-31-10-134 k8s-specifications]# kubectl get po
NAME                                 READY   STATUS    RESTARTS   AGE
db-b54cd94f4-ntqpb                   1/1     Running   0          2d10h
redis-868d64d78-xpr4p                1/1     Running   0          2d10h
result-5d57b59f4b-5q28v              1/1     Running   0          2d10h
vote-94849dc97-qccjd                 1/1     Running   0          18s
worker-dd46d7584-wwchb               1/1     Running   1          2d10h

Observation-> Here as we can see the voting pod is deleted there is new voting pod created as this pod is replica. The application has no impact because of the voting pod deletion.
 
 #####################################################################################   
 
Deleting the Worker Pod:

[root@ip-172-31-10-134 k8s-specifications]# kubectl get po
NAME                                 READY   STATUS    RESTARTS   AGE
db-b54cd94f4-ntqpb                   1/1     Running   0          2d10h
redis-868d64d78-xpr4p                1/1     Running   0          2d10h
result-5d57b59f4b-5q28v              1/1     Running   0          2d10h
vote-94849dc97-m7x2p                 1/1     Running   0          56s
worker-dd46d7584-wwchb               1/1     Running   1          2d10h

[root@ip-172-31-10-134 k8s-specifications]# kubectl delete pod worker-dd46d7584-wwchb
pod "worker-dd46d7584-wwchb" deleted

[root@ip-172-31-10-134 k8s-specifications]# kubectl get po
NAME                                 READY   STATUS    RESTARTS   AGE
db-b54cd94f4-ntqpb                   1/1     Running   0          2d10h
redis-868d64d78-xpr4p                1/1     Running   0          2d10h
result-5d57b59f4b-5q28v              1/1     Running   0          2d10h
vote-94849dc97-m7x2p                 1/1     Running   0          113s
worker-dd46d7584-wlnwb               1/1     Running   0          39s

Observation: - A new pod is created as it is a replica , Voting app can be excessed and the flow is also not disturbed , all the other app (voting,redis,dp,result) worked fine

########################################################################################

Deleting the DB pod
  
kubectl delete pod db-b54cd94f4-ntqpb
kubectl get pod
  
Observation:

Once the DB pod is deleted new instance of the DB pod is created.
->> There is a impact in the application. Where the Voting data is stored in the DB but The same is not reflected in the Result Page.
->> The Reason for the data not updating in the result page is that the when the DB Pod was deleted the socket connection towards the Result pod got disconnected.
->>The new DB pod which was created did not connect to the Result pod, because of the coding issue in the Application as it was not handled.

###########################################################################################

Solution:
--> To fix the issue with the application --  restart the Result Pod or Delete the Result Pod.
--> Once the Result pod instance is recreated then the socket connection is restored with the DB pod and the Application starts to work as expected.
--> Even though the Application is up and running, the Previous voting data which was stored in the DB pod is lost due to the deletion of the DB pod.

  
